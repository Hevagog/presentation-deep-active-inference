## Quick Notation Note
Please note that from now on we will be using different notation to be more familiar with notation found in reinforcement learning, where $o_\tau$ is the observation at time $\tau$, $s_\tau$ is the hidden state at time $\tau$, and $t$ is the current time step.  
From this we can draw a parallel to the Active Inference framework by noting that the generative model $P(o_\tau|s_\tau)$ is equivalent to the likelihood mapping $P(y|x)$, where $y$ is the observation and $x$ is the hidden state:
$$
P(y,x) \equiv P(o_\tau,s_\tau)
$$
where $y$ is the observation and $o_\tau$ is the observation at time $\tau$, and $x$ is the hidden state and $s_\tau$ is the hidden state at time $\tau$.

---

## Expected Free Energy

- Expected Free Energy (EFE) extends Active Inference to include prospective form of cognition: **planning**.  
- Planning a sequence of actions requires considering future observations that will be generated by those actions.
- Each possible sequence of actions is termed a **policy**.  
- Active Inference treats planning and decision-making as a process of inferring what to do, which brings planning into the realm of Bayesian inference.  
- In active inference, agents choose an action given by their EFE. In particular, any given action is selected with a probability proportional to the accumulated negative EFE of the corresponding policies $G(\pi)$.

## Expected Free Energy (EFE) 

We define **Expected Free Energy** $G$ for a policy $\pi$ as:
$$
G(\pi)=\sum_{\tau=t+1}^T G_\tau(\pi) %= \sum_{\tau =t+1}^T E_{P(o_\tau|s_\tau)Q(s_\tau|\pi)} [\ln Q(s_\tau|\pi)-\ln P(o_\tau|s_\tau)]
$$ {#eq:expected-free-energy}
where $G_\tau(\pi)$ is the expected free energy at time $\tau$.  

---

$G_\tau(\pi)$ can be decomposed into two components:
$$
\begin{aligned}
G_\tau(\pi) &= \mathbb{E}_{P(o_\tau|s_\tau) Q(s_\tau|\pi)} [\ln Q(s_\tau|\pi) - \ln P(o_\tau, s_\tau)] \\
&= \mathbb{E}_{P(o_\tau|s_\tau) Q(s_\tau|\pi)} [\ln Q(s_\tau|\pi) - \ln P(s_\tau|o_\tau) - \ln P(o_\tau)] \\
&\approx \mathbb{E}_{P(o_\tau|s_\tau) Q(s_\tau|\pi)} [\ln Q(s_\tau|\pi) - \ln Q(s_\tau) - \ln P(o_\tau)] \quad \text{(using } Q(s_\tau) \approx P(s_\tau|o_\tau) \text{)} \\
&= \underbrace{D_{\text{KL}} [Q(s_\tau|\pi) \| Q(s_\tau)]}_{\text{Epistemic Value (Information Gain)}} - \underbrace{\mathbb{E}_{P(o_\tau|s_\tau) Q(s_\tau|\pi)} [\ln P(o_\tau)]}_{\text{Extrinsic Value (Log Preference)}}
\end{aligned}
$$ {#eq:efe-decomposition}
where $Q$ is the **approximate posterior** (variational/recognition distribution), parameterized by $\phi$; $P$ is the **generative model**; $\mathbb{E}_{P(o_\tau|s_\tau) Q(s_\tau|\pi)}$ is the expectation over the joint distribution of observations and hidden states, which captures the agent's predictions about future observations and states when following a specific policy.

---

- **Epistemic Value**: 
  - encourages exploration by reducing uncertainty in state transitions. 
  - This value measures how much posterior beliefs $Q(s_\tau|\pi)$ (after following $\pi$) would differ from the prior beliefs $Q(s_\tau)$â€”i.e., the expected reduction in uncertainty
  - A high epistemic value means the chosen policy is expected to yield observations that greatly clarify unknown aspects of the world.

- **Extrinsic Value**: 
  - encourages exploitation by maximizing the expected log preference of the observations.
  - This value measures how much the expected observations $P(o_\tau)$ are preferred by the agent.
  - A high extrinsic value means the chosen policy is expected to yield observations that are highly preferred by the agent.

When uncertainty is high, i.e. variational distribution $Q(s_\tau)$ is far from the prior $P(s_\tau|o_\tau)$, the epistemic value will dominate the expected free energy, driving exploration.

Once the agent has learned about the environment, i.e. the variational distribution $Q(s_\tau)$ is close to the prior $P(s_\tau|o_\tau)$, the extrinsic value will dominate, driving exploitation.

---

As earlier stated, action is selected with a probability proportional to the accumulated negative EFE of the corresponding policies $G(\pi)$ ([@eq:expected-free-energy]). However, computing $G(\pi)$ for all possible policies is computationally expensive, since it involves making an exponentially-increasing number of predictions for $T$ -steps into the future, and computing all the terms.  

Another major issue is that during the calculation of $G(\pi)$, the agent needs to calculate intractable distribution of $P(o_\tau)$.

The question arises: **How can we compute $G(\pi)$ efficiently?**